{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "202d99b2",
   "metadata": {},
   "source": [
    "# Load Testing Datasets Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336f4d52",
   "metadata": {},
   "source": [
    "- First of all we need to import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fceee7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from faker import Faker\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "Faker.seed(0) # For reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcc0601",
   "metadata": {},
   "source": [
    "- We will read the locations (unique city & state combinations for which we have businesses in the dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c20a97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_50k = pd.read_csv('./data/locations_50k.csv').locations\n",
    "locations_100k = pd.read_csv('./data/locations_100k.csv').locations\n",
    "locations = pd.read_csv('./data/locations_full.csv').locations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aee44f4",
   "metadata": {},
   "source": [
    "- We will create an instance of the `Nominatim` class from the `geopy` module, to geocode the locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68a261ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "geolocator = Nominatim(user_agent=\"radius_query_benchmarking\", timeout=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b46c28a",
   "metadata": {},
   "source": [
    "- We will iterate over the locations and geocode each location to retrieve its latitude-longitude pair.\n",
    "- This will take about ~10 minutes to complete because the `Nominatim` geocoder has a request limit, however, it is convenient because it does not require an API key like other geocoders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "621b3455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to geocode Nashville-Davidson metropolitan government (balance), TN\n",
      "Failed to geocode Webster Grvs, MO\n",
      "Failed to geocode Redingtn Shor, FL\n",
      "Failed to geocode New Prt Rchy, FL\n",
      "Failed to geocode Staint Albert, AB\n",
      "Failed to geocode Indianpolis, IN\n",
      "Failed to geocode TWN N CNTRY, FL\n",
      "Failed to geocode Holland Southampton, PA\n",
      "Failed to geocode Royford, PA\n",
      "Failed to geocode Twn N Cntry, FL\n",
      "Failed to geocode Hadden, NJ\n",
      "Failed to geocode Mehville, MO\n",
      "Failed to geocode West Deptford Townsh, NJ\n",
      "Failed to geocode Mc Cordsville, IN\n",
      "Failed to geocode Creve Couer, MO\n",
      "Failed to geocode Mount Juliet, TX\n",
      "Failed to geocode Pennsaulen, NJ\n",
      "Failed to geocode Thonosassa, FL\n",
      "Failed to geocode Glenoldan, PA\n",
      "Failed to geocode Had Twp, NJ\n",
      "Failed to geocode Westmont - Haddon Towsship, NJ\n",
      "Failed to geocode Zephryhills, FL\n",
      "Failed to geocode VC Highlands, NV\n",
      "Failed to geocode St. Loius, MO\n",
      "Failed to geocode Belleair Blf, FL\n",
      "Failed to geocode Cherry Hil, NJ\n",
      "Failed to geocode Claerwater, FL\n",
      "Failed to geocode Phonixville, PA\n",
      "Failed to geocode Pilot Sound Area West Portion, AB\n",
      "Failed to geocode Hamiltion, NJ\n",
      "Failed to geocode King of Prussi, PA\n",
      "Failed to geocode Drexel Hil, PA\n",
      "Failed to geocode Brentwood - Cool Springs, TN\n",
      "Failed to geocode Conshohoeken, PA\n",
      "Failed to geocode Norritown, PA\n",
      "Failed to geocode Sturgeon Couny, AB\n",
      "Failed to geocode Mount Laurel, NV\n",
      "Failed to geocode Belleair Blufs, FL\n",
      "Failed to geocode Chalemette, LA\n",
      "Failed to geocode Evshm Twp, NJ\n",
      "Failed to geocode Feasterville Trev, PA\n",
      "Failed to geocode Delray Beach, IN\n",
      "Failed to geocode N Redngtn Bch, FL\n",
      "Failed to geocode Tampa, SD\n",
      "Failed to geocode Greater Carrollwood, FL\n",
      "Failed to geocode SANTA BARBARA AP, CA\n",
      "Failed to geocode Tampla, FL\n",
      "Failed to geocode Nolenville, TN\n",
      "Failed to geocode Festerville, PA\n",
      "Failed to geocode Liverpool, XMS\n",
      "Failed to geocode Goodletsville, TN\n",
      "Failed to geocode Redingtn Shores, FL\n",
      "Failed to geocode Philiidelphia, PA\n",
      "Failed to geocode New Britian, PA\n",
      "Failed to geocode Nsshville, TN\n",
      "Failed to geocode Newtown Sqaure, PA\n",
      "Failed to geocode Indianapolis city (balance), IN\n",
      "Failed to geocode Berlin Boro, NJ\n",
      "Failed to geocode Maryland Height, MO, MO\n",
      "Failed to geocode Wesley Ccapel, FL\n",
      "Failed to geocode Green Valle, AZ\n",
      "Failed to geocode Plainfiled, IN\n",
      "Failed to geocode Tierre Verde, FL\n",
      "Failed to geocode Maran, AZ\n",
      "Failed to geocode Landsale, PA\n",
      "Failed to geocode Foster Pond, IL\n",
      "Failed to geocode UPPR BLCK EDY, PA\n",
      "Failed to geocode Haverton, PA\n",
      "Failed to geocode INpolis, IN\n",
      "Failed to geocode Kng Of Prusia, PA\n",
      "Failed to geocode Safety Hatbor, FL\n",
      "Failed to geocode Maryland Height, MO\n",
      "Failed to geocode PLYMOUTH MTNG, PA\n"
     ]
    }
   ],
   "source": [
    "center_lat_long_pairs_50k = []\n",
    "center_lat_long_pairs_100k = []\n",
    "center_lat_long_pairs = []\n",
    "for location in locations:\n",
    "    location_data = geolocator.geocode(location)\n",
    "\n",
    "    if location_data is not None:\n",
    "        center_latitude = location_data.latitude\n",
    "        center_longitude = location_data.longitude\n",
    "        center_lat_long_pairs.append((float(center_latitude), float(center_longitude)))\n",
    "        \n",
    "        if location in locations_50k.values:\n",
    "            center_lat_long_pairs_50k.append((float(center_latitude), float(center_longitude)))\n",
    "        \n",
    "        if location in locations_100k.values:\n",
    "            center_lat_long_pairs_100k.append((float(center_latitude), float(center_longitude)))\n",
    "    else:\n",
    "        print(f\"Failed to geocode {location}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd5c4ef",
   "metadata": {},
   "source": [
    "- We will create an instance of the `Faker` class from the `faker` module, to generate data around the centers of the geocoded locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35d4cffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = Faker()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9ede8c",
   "metadata": {},
   "source": [
    "- We will generate 15.000 latitude-longitude pairs around the centers of the geocoded locations for each of the datasets, to be used in the load testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27583188",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_stores_load_test_data(center_lat_long_pairs, num_of_samples):\n",
    "    latitudes = []\n",
    "    longitudes = []\n",
    "    max_distances = []\n",
    "    for i, (lat, long) in enumerate(itertools.cycle(center_lat_long_pairs)):\n",
    "        latitudes.append(f.coordinate(lat, radius=0.02))\n",
    "        longitudes.append(f.coordinate(long, radius=0.02))\n",
    "        max_distances.append(f.random_int(min=1, max=5))\n",
    "        if i == num_of_samples:\n",
    "            break\n",
    "\n",
    "    return latitudes, longitudes, max_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e480479b",
   "metadata": {},
   "outputs": [],
   "source": [
    "latitudes_50k, longitudes_50k, max_distances_50k = \\\n",
    "    generate_stores_load_test_data(center_lat_long_pairs_50k, 15_000)\n",
    "\n",
    "latitudes_100k, longitudes_100k, max_distances_100k = \\\n",
    "    generate_stores_load_test_data(center_lat_long_pairs_100k, 15_000)\n",
    "\n",
    "latitudes, longitudes, max_distances = \\\n",
    "    generate_stores_load_test_data(center_lat_long_pairs, 15_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfa0087",
   "metadata": {},
   "source": [
    "- We will create a load testing DataFrame for each of the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57bee4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_stores_params_df_50k = pd.DataFrame({\n",
    "    'latitude': latitudes_50k,\n",
    "    'longitude': longitudes_50k,\n",
    "    'max_distance': max_distances_50k\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a849d05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_stores_params_df_100k = pd.DataFrame({\n",
    "    'latitude': latitudes_100k,\n",
    "    'longitude': longitudes_100k,\n",
    "    'max_distance': max_distances_100k\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b4270ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_stores_params_df = pd.DataFrame({\n",
    "    'latitude': latitudes,\n",
    "    'longitude': longitudes,\n",
    "    'max_distance': max_distances\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31e0e45",
   "metadata": {},
   "source": [
    "- We will split the `search_stores_params_df_50k`, `search_stores_params_df_100k` & `search_stores_params_df` into 3 parts and we will save each of them as csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d57900d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stores_load_test_data_to_csv(df, dataset_size, vus=[50, 100, 200]):\n",
    "    split_points = np.quantile(df.index, [1/3, 2/3])\n",
    "    split_point_1, split_point_2 = round(split_points[0]), round(split_points[1])\n",
    "\n",
    "    df[:split_point_1].to_csv(f'./datasets/search_stores_params_{dataset_size}_{vus[0]}vu.csv', index=False)\n",
    "    df[split_point_1:split_point_2].to_csv(f'./datasets/search_stores_params_{dataset_size}_{vus[1]}vu.csv', index=False)\n",
    "    df[split_point_2:].to_csv(f'./datasets/search_stores_params_{dataset_size}_{vus[2]}vu.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c723546",
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_load_test_data_to_csv(search_stores_params_df_50k, \n",
    "                             dataset_size='50k')\n",
    "stores_load_test_data_to_csv(search_stores_params_df_100k, \n",
    "                             dataset_size='100k')\n",
    "stores_load_test_data_to_csv(search_stores_params_df, \n",
    "                             dataset_size='full')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d065bffe",
   "metadata": {},
   "source": [
    "- We will generate 6.000 latitude-longitude pairs around the centers of the geocoded locations for each of the datasets and data to filter the products by price and calories, to be used in the load testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e75609a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_products_load_test_data(center_lat_long_pairs, num_of_samples):\n",
    "    latitudes = []\n",
    "    longitudes = []\n",
    "    max_distances = []\n",
    "    min_prices = []\n",
    "    max_prices = []\n",
    "    min_calories = []\n",
    "    max_calories = []\n",
    "    for i, (lat, long) in enumerate(itertools.cycle(center_lat_long_pairs)):\n",
    "        latitudes.append(f.coordinate(lat, radius=0.02))\n",
    "        longitudes.append(f.coordinate(long, radius=0.02))\n",
    "        max_distances.append(f.random_int(min=1, max=5))\n",
    "        min_prices.append(f.random_int(min=0, max=3))\n",
    "        max_prices.append(f.random_int(min=4, max=7))\n",
    "        min_calories.append(f.randomize_nb_elements(number=150))\n",
    "        max_calories.append(f.randomize_nb_elements(number=600))\n",
    "        if i == num_of_samples:\n",
    "            break\n",
    "\n",
    "    return (latitudes, longitudes, max_distances, \n",
    "            min_prices, max_prices, min_calories, max_calories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a95e3ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "(latitudes_50k, longitudes_50k, max_distances_50k, \n",
    " min_prices_50k, max_prices_50k, min_calories_50k, max_calories_50k) = \\\n",
    "    generate_products_load_test_data(center_lat_long_pairs_50k, 6000)\n",
    "\n",
    "(latitudes_100k, longitudes_100k, max_distances_100k, \n",
    " min_prices_100k, max_prices_100k, min_calories_100k, max_calories_100k) = \\\n",
    "    generate_products_load_test_data(center_lat_long_pairs_100k, 6000)\n",
    "\n",
    "(latitudes, longitudes, max_distances, \n",
    " min_prices, max_prices, min_calories, max_calories) = \\\n",
    "    generate_products_load_test_data(center_lat_long_pairs, 6000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6643156",
   "metadata": {},
   "source": [
    "- We will create a a load testing DataFrame for each of the datasetsâ€‹."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd5954ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_products_params_df_50k = pd.DataFrame({\n",
    "    'latitude': latitudes_50k,\n",
    "    'longitude': longitudes_50k,\n",
    "    'max_distance': max_distances_50k,\n",
    "    'min_price': min_prices_50k,\n",
    "    'max_price': max_prices_50k,\n",
    "    'min_calories': min_calories_50k,\n",
    "    'max_calories': max_calories_50k\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36509725",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_products_params_df_100k = pd.DataFrame({\n",
    "    'latitude': latitudes_100k,\n",
    "    'longitude': longitudes_100k,\n",
    "    'max_distance': max_distances_100k,\n",
    "    'min_price': min_prices_100k,\n",
    "    'max_price': max_prices_100k,\n",
    "    'min_calories': min_calories_100k,\n",
    "    'max_calories': max_calories_100k\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "80008870",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_products_params_df = pd.DataFrame({\n",
    "    'latitude': latitudes,\n",
    "    'longitude': longitudes,\n",
    "    'max_distance': max_distances,\n",
    "    'min_price': min_prices,\n",
    "    'max_price': max_prices,\n",
    "    'min_calories': min_calories,\n",
    "    'max_calories': max_calories\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b10f5c",
   "metadata": {},
   "source": [
    "- We will split the `search_products_params_df_50k`, `search_products_params_df_100k` & `search_products_params_df` into 3 parts and we will save each of them as csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "26b69865",
   "metadata": {},
   "outputs": [],
   "source": [
    "def products_load_test_data_to_csv(df, dataset_size, vus=[25, 50, 100]):\n",
    "    split_points = np.quantile(df.index, [1/3, 2/3])\n",
    "    split_point_1, split_point_2 = round(split_points[0]), round(split_points[1])\n",
    "\n",
    "    df[:split_point_1].to_csv(f'./datasets/search_products_params_{dataset_size}_{vus[0]}vu.csv', index=False)\n",
    "    df[split_point_1:split_point_2].to_csv(f'./datasets/search_products_params_{dataset_size}_{vus[1]}vu.csv', index=False)\n",
    "    df[split_point_2:].to_csv(f'./datasets/search_products_params_{dataset_size}_{vus[2]}vu.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f673240c",
   "metadata": {},
   "outputs": [],
   "source": [
    "products_load_test_data_to_csv(search_products_params_df_50k, \n",
    "                               dataset_size='50k')\n",
    "products_load_test_data_to_csv(search_products_params_df_100k, \n",
    "                               dataset_size='100k')\n",
    "products_load_test_data_to_csv(search_products_params_df, \n",
    "                               dataset_size='full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fba73c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
